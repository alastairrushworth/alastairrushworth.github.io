<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>alastair rushworth - Data Science</title>
    <subtitle>a blog about data science and other things</subtitle>
    <link rel="self" type="application/atom+xml" href="https://alastairrushworth.github.io/categories/data-science/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://alastairrushworth.github.io/"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2024-06-29T00:00:00+00:00</updated>
    <id>https://alastairrushworth.github.io/categories/data-science/atom.xml</id>
    <entry xml:lang="en">
        <title>On using code copilots in data science</title>
        <published>2024-06-29T00:00:00+00:00</published>
        <updated>2024-06-29T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://alastairrushworth.github.io/posts/on-using-copilots-in-data-science/"/>
        <id>https://alastairrushworth.github.io/posts/on-using-copilots-in-data-science/</id>
        
        <content type="html" xml:base="https://alastairrushworth.github.io/posts/on-using-copilots-in-data-science/">&lt;p&gt;I‚Äôve lost track of how long I‚Äôve been using &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;features&#x2F;copilot&quot;&gt;Github‚Äôs Copilot&lt;&#x2F;a&gt; via the &lt;a href=&quot;https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=GitHub.copilot&quot;&gt;VSCode extension&lt;&#x2F;a&gt; but it‚Äôs well over 18 months at this point. It‚Äôs been absolutely game-changing for my productivity and enjoyment of writing code, but I‚Äôm often surprised to find that other data scientists haven‚Äôt tried it (or an equivalent tool like &lt;a href=&quot;https:&#x2F;&#x2F;www.qodo.ai&#x2F;&quot;&gt;codium&lt;&#x2F;a&gt;, &lt;a href=&quot;https:&#x2F;&#x2F;www.tabnine.com&#x2F;&quot;&gt;tabnine&lt;&#x2F;a&gt; and &lt;a href=&quot;https:&#x2F;&#x2F;www.cursor.com&#x2F;&quot;&gt;cursor.sh&lt;&#x2F;a&gt;). I‚Äôve writing this to explain why I think it‚Äôs so important, and that it should be considered an essential part of a data scientist‚Äôs toolkit. If you don‚Äôt use a code copilot tool, I hope this provides a perspective on what you might be missing. If you do already, hopefully it resonates with your experiences.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;writing-code-is-slow&quot;&gt;Writing code is slow&lt;&#x2F;h2&gt;
&lt;p&gt;The physical act of typing is an order of magnitude slower than the time it takes to decide what you intend to write. While typing, you need to remember how to use tools correctly to execute your intent (&lt;em&gt;how does pandas &lt;code&gt;.pivot()&lt;&#x2F;code&gt; work again?&lt;&#x2F;em&gt;), probably google some syntax (üíÄ &lt;em&gt;Stack Overflow&lt;&#x2F;em&gt;) and fix bugs or mistakes (&lt;em&gt;ugh, forgot to &lt;code&gt;.reset_index&lt;&#x2F;code&gt;&lt;&#x2F;em&gt;). There‚Äôs also exploratory work and data analysis &amp;amp; also iterating and refactoring (&lt;code&gt;that didn‚Äôt work, let‚Äôs try something else&lt;&#x2F;code&gt;), docstrings, comments, editing YAML files, yada yada. To say nothing of starting a new project and firing up your favourite modules and writing a first implementation of something.&lt;&#x2F;p&gt;
&lt;p&gt;All of this is just slow, compared to the time it takes to imagine what you are going to do. Ok, I know some of you can touch type, have black-belts in vim shortcuts and retain code docs in your photographic memories. But a majority of data science development time is spent punching out pretty standard code, and happily, this is exactly the type of code that copilots are excellent at anticipating with very good suggestions.&lt;&#x2F;p&gt;
&lt;p&gt;A slight philosophical segue. I think there‚Äôs an important distinction to be made between &lt;em&gt;typing code&lt;&#x2F;em&gt;, which is something you do with your fingers (and your web browser), and &lt;em&gt;realising ideas with software&lt;&#x2F;em&gt; which is something you mostly do with your brain. I‚Äôm not sure you can do one without the other, and the distinction isn‚Äôt totally crisp ‚Äî but bear with me.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;code-is-cheap-thinking-is-expensive&quot;&gt;Code is cheap, thinking is expensive&lt;&#x2F;h2&gt;
&lt;p&gt;Something we don‚Äôt talk about enough is just how much of the code we write ends up being thrown away. Projects change and sometimes die, ideas evolve and so does our code. But many (many) written lines of development, debugging, and EDA code are ephemeral and never even seen by another person.&lt;&#x2F;p&gt;
&lt;p&gt;It‚Äôs a mistake to think of discarded code as waste, it‚Äôs an essential part of development. As many authors have observed, &lt;a href=&quot;https:&#x2F;&#x2F;alistapart.com&#x2F;article&#x2F;writing-is-thinking&#x2F;&quot;&gt;writing is thinking&lt;&#x2F;a&gt; and writing code is no different. This is most obvious to me in the case of &lt;a href=&quot;https:&#x2F;&#x2F;medium.com&#x2F;@alastairmrushworth&#x2F;exploratory-data-analysis-whats-the-point-56c73d33ec73&quot;&gt;EDA, where we learn by coding, thinking and iterating&lt;&#x2F;a&gt; and much of this code doesn‚Äôt see the light of day. There‚Äôs more in the linked article, but I don‚Äôt at all think of EDA as an ‚Äòanalysis stage‚Äô as it‚Äôs often framed, but more of process of thinking and investigation.&lt;&#x2F;p&gt;
&lt;p&gt;Anyways, finished code is a digital manifestation of an idea, and writing the code is a form of thinking that leads to that manifestation. There‚Äôs a ton of value in the thinking part ‚Äî in general, more thinking should result in better ideas and finished software. Time is always a regularising factor on how much of this type of work can take place. Copilots act as an accelerant and multiplier that makes delivering better ideas easier, faster and maybe even delightful.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;so-what-gives&quot;&gt;So what gives?&lt;&#x2F;h2&gt;
&lt;p&gt;Why the resistance the uptake of this type of tool? There‚Äôre a few reasons I‚Äôve observed. The main one I think is that it‚Äôs simply passed a lot of people by ‚Äî the last 2 years have passed in a haze of loud AI hype and chatbots. During the same period of time, code copilots have gone from being interesting, cool toys to something completely game-changing. You only need to scan some of the [comments on hackernews when Github Copilot first launched](the comments on hackernews when Github Copilot first launched) to get a sense of what a step change it was.&lt;&#x2F;p&gt;
&lt;p&gt;Another reasonable objection is the risk of wrong &#x2F; hallucinated code inadvertently getting pushed and causing issues. To anyone worried about this, I suggest trying one of the major copilots out for a while, the risk is much lower than one might imagine. The workflow prevents this to an extent ‚Äî copilots are more like very smart autocomplete, where you have full control over whether a code suggestion is accepted or not. It‚Äôs not at all like blindly copy-pasting large code generations from ChatGPT (though I believe this also has a place, but that‚Äôs for a another article).&lt;&#x2F;p&gt;
&lt;p&gt;Speaking from personal experience, I used to be a bit precious about my code, and definitely felt defensive about the idea of copilots when I first started playing with &lt;a href=&quot;https:&#x2F;&#x2F;www.tabnine.com&#x2F;&quot;&gt;tabnine&lt;&#x2F;a&gt; (which has been around longer than most). I‚Äôm not sure but I think this attitude is fairly common and is probably reinforced by the emphasis many companies put on squeaky clean SWE practises when hiring. The myth of the 10x engineer polyglot who can write the full stack and do deep specialist development definitely doesn‚Äôt help either. There‚Äôs probably a lot more to it than that, but I hope you know what I‚Äôm talking about. I think all of these cultural threads add up to a kind of jealousness over our hard-won skills that results in a reflexive rejection of tools that might displace them. I sometimes need to remind myself that over sufficiently long time scales, much of our knowledge of syntax will be made redundant anyways ‚Äî in 10 years time, I expect that half of the modules I routinely use now will have changed or been updated beyond recognition. Bottom line is that it‚Äôs good to care about code, but don‚Äôt let it get in the way of trying new ways to do it.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;wrap-up-give-it-a-go&quot;&gt;Wrap up: give it a go&lt;&#x2F;h2&gt;
&lt;p&gt;My best advice would be to try out a copilot. I really like Github‚Äôs, because it integrates seamlessly with VSCode and it really hasn‚Äôt missed a beat since I first subscribed. It‚Äôs absolutely the easiest $10 I spend each month.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Exploratory Data Analysis: what‚Äôs the point?</title>
        <published>2020-05-12T00:00:00+00:00</published>
        <updated>2020-05-12T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://alastairrushworth.github.io/posts/exploratory-data-analysis-whats-the-point/"/>
        <id>https://alastairrushworth.github.io/posts/exploratory-data-analysis-whats-the-point/</id>
        
        <content type="html" xml:base="https://alastairrushworth.github.io/posts/exploratory-data-analysis-whats-the-point/">&lt;p&gt;Exploratory data analysis or EDA is one of the most important but difficult to codify parts of the data science toolkit. True exploratory analysis is without a sharply definable objective and evades being formalised into a set of clear steps. Despite this, EDA is used in at least a few very typical ways that connect to downstream tasks like data cleaning and hypothesis generation. But perhaps most importantly, it‚Äôs an integral part of how we learn to frame our thinking as data scientists. This post attempts to offer some perspective on the less-discussed ways in which EDA develops our contextual understanding of a data analysis.&lt;&#x2F;p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;&#x2F;images&#x2F;eda_1.png&quot; alt=&quot;drawing&quot; width=&quot;700&quot; style=&quot;display: block; margin: 0 auto;&quot;&#x2F;&gt;
&lt;&#x2F;div&gt;
&lt;h2 id=&quot;eda-for-checking-validation-and-cleaning&quot;&gt;EDA for checking, validation and cleaning&lt;&#x2F;h2&gt;
&lt;p&gt;Let‚Äôs get the obvious stuff out of the way first. Where a rough analysis plan is already in place, and some data has been assembled to support the analysis, a type of EDA serves to identify potential issues that might require remedial work before progressing. This is probably the most common type of exploratory analysis and is more closely linked to the goals of data cleaning than pure analysis and insight. This a big topic and I won‚Äôt attempt an exhaustive list here, but instead will describe a few of the most common tasks.&lt;&#x2F;p&gt;
&lt;p&gt;The most common check is for the correctness of column types. Depending on the data source, different issues might arise here, but you‚Äôll be familiar with at least some of these. Integers incorrectly encoded as strings, strings encoded as dates, unordered categories encoded as integers. Sometimes a column that should be numeric has the very occasional string entry. There are as many causes as there are issues: perhaps you didn‚Äôt specify the correct schema when you read the data; or the data are encoded in ambiguous way that results in an inappropriate type; or maybe some earlier data manipulation induced an unintended problem.&lt;&#x2F;p&gt;
&lt;p&gt;We often check the prevalence of missing values and their dependence on other important features ‚Äî usually because a lot of analysis methods do not handle missing values natively. Some columns may be totally unusable if they are mostly missing. Remedies here might include dropping or transform columns, imputing missing values, or choosing an algorithm that handles missingness out of the box.&lt;&#x2F;p&gt;
&lt;p&gt;Distribution, shift and relevance: it is important to inspect the distribution of values in each column ‚Äî and consider whether these look how we‚Äôd expect (where we have an expectation). Do the distributions covary, especially with time (data are almost never consistent with stationarity with respect to time). Thinking about distributional shift is crucial for making decisions around which window of data is most important or relevant for addressing a specific question. It might expose or confirm trends and temporal patterns that downstream analysis needs to be aware of.&lt;&#x2F;p&gt;
&lt;p&gt;Measuring pairwise association provides some basic insights into how columns covary and might help reveal columns that are collinear or even identical that could be removed without detriment. It might help uncover some of the overall structure in the data or indicate collections of related columns. Pairwise association measures, like Pearson correlation coefficients, are overused in this context and are limited to only providing a linear and unconditional view of pairwise association. Nevertheless, a lot of insight can be gleaned from this type of analysis if you know what to look for.&lt;&#x2F;p&gt;
&lt;p&gt;These types of techniques provide a first look at the data and answer important questions about quality, formatting and overall dependence structure. These steps can usually be carried out by the data analyst without any external support, and are generally well supported with easy-to-use code wrappers. These are absolutely essential steps and it‚Äôs possible to learn quite a lot about the data by applying them and thinking carefully about the results. But it‚Äôs very important to recognise that there is a limit to how much can be understood with this type of analysis. There‚Äôs a lot more to EDA.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;grokking-the-data-with-eda&quot;&gt;grokking the data with EDA&lt;&#x2F;h2&gt;
&lt;blockquote&gt;
&lt;p&gt;When you claim to ‚Äúgrok‚Äù some knowledge or technique, you are asserting that you have not merely learned it in a detached instrumental way but that it has become part of you, part of your identity.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http:&#x2F;&#x2F;www.catb.org&#x2F;~esr&#x2F;jargon&#x2F;html&#x2F;G&#x2F;grok.html&quot;&gt;‚Äògrok‚Äô, &lt;em&gt;The Jargon File&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;I‚Äôve totally made up the heading, but I think it‚Äôs by far the most important role of EDA and is mostly what the rest of this post is about. There is a sort of myth of the data analyst as a robotic processor of data, who is detached and passive. The reality is completely the opposite, where better data analysis will always come from an analyst with a deep understanding of the data and the processes that generated it. EDA has a crucial role in turning a data frame from a contextless collection of bytes into a meaningful representation of a physical process, transitioning the analyst from the passive processor to an expert with deeply internalised understanding of an area. This end state is intangible and qualitative because it happens completely in your own head. Consequently, this part of the EDA will be a creative and personal journey that is supported by a continuing internal conversation that probes and revisits your understanding of the broader context.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;building-a-data-narrative&quot;&gt;Building a data narrative&lt;&#x2F;h2&gt;
&lt;p&gt;The data frame you have in front of you for analysis is an incomplete and encoded representation of some real world process. Part of your role as an analyst is to solve problems and generate insights that respects the story of how the data were generated. For want of a better description, let‚Äôs call this story the data narrative. Part of this narrative might be the sequencing of events that lead to each data record coming into existence, part of it might be the data‚Äôs lineage in terms of the processing, joins and wrangling required to produce the data frame you end up with. If you are already an expert in the area you are working, this narrative may already be engrained in you. The data narrative completely frames the work you do, how you interpret every insight or modeled output, and most importantly, the credibility with which you can influence your audience.&lt;&#x2F;p&gt;
&lt;p&gt;The data narrative is a complex form of metadata and is almost never part of the data frame. If you are fortunate, your organisation might keep clear and accessible documentation and data dictionaries that will be a huge first step to piecing together this narrative. However, it is often more typical that analysts are neither domain experts nor well-provided with nice documentation. In this case, the narrative is something that must be synthesised through detective work, drawing on a combination of data analysis and the experience of domain experts. This is, of course, much easier said than done.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-role-of-asking-questions&quot;&gt;The role of asking questions&lt;&#x2F;h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Your goal during EDA is to develop an understanding of your data. The easiest way to do this is to use questions as tools to guide your investigation. When you ask a question, the question focuses your attention on a specific part of your dataset and helps you decide which graphs, models, or transformations to make.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;r4ds.had.co.nz&#x2F;exploratory-data-analysis.html&quot;&gt;&lt;em&gt;R for Data Science&lt;&#x2F;em&gt;, Hadley Wickham (2021)&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;EDA can‚Äôt happen in a computational vacuum. To do this well, we need to be alternating between interrogating the data and asking ourselves if what we find is consistent with our internal understanding of the data‚Äôs narrative. &lt;em&gt;Does what I see make sense to me? Would I feel comfortable to explain it to someone else?&lt;&#x2F;em&gt;#&lt;&#x2F;p&gt;
&lt;p&gt;In large organisations, you might also be speaking with a domain expert to help with this (if that person isn‚Äôt you), though they needn‚Äôt be an internal expert if the data come from outside the organisation. If you don‚Äôt yet have direct access to such a person, demand that you do ‚Äî this person will supercharge your eventual analysis and will be often be the difference between success or failure of the entire project. In the beginning, take lots of time to let experts talk more broadly about the data, as they understand all of the salient dependencies, anomalies and gotchas that will save you a lot of time in the long run. Take time to use simple data analysis to carefully confirm what you‚Äôre told. The key here is not checking for correctness, but to grow your understanding of the data: it‚Äôs important to remember that it‚Äôs one thing to be told something about the data narrative, but it‚Äôs much more meaningful to use your own analysis to see it expressed in the data.&lt;&#x2F;p&gt;
&lt;p&gt;As your understanding deepens and the analysis progresses, you‚Äôll continue to find new patterns and structure in the data. Keep revisiting your understanding of the data narrative, and check whether what you are seeing is consistent with that. As your understanding of the data narrative matures, the gaps will come into focus: consider creative ways to use the data or ask a relevant question to close the gap. The relationship between internalised data narrative and data exploration is a two-way street.&lt;&#x2F;p&gt;
&lt;p&gt;Take time to talk your findings over with another data scientist. The key here is to aim to communicate your understanding of the data narrative without getting too mired in the technical details of the data. The process of preparing a narrative that you can explain to a colleague will help to consolidate what you‚Äôve learned and quickly expose gaps. A fresh set of eyes will nearly always raise further questions or force you to think of your data from a different perspective.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;do-we-even-have-the-right-data&quot;&gt;Do we even have the right data?&lt;&#x2F;h2&gt;
&lt;p&gt;An important byproduct of the process of building a better data narrative is that your understanding of what the most important or relevant questions to ask will improve. A crucial question to keep revisiting is whether the data you have is sufficient to address the most important questions. Are there additional data sources that you draw upon to enrich or improve the analysis? Are the columns you already have in your data frame defined correctly, or should they really be specified differently? It‚Äôs typical that data sets are assembled before anyone knows exactly how the data will be used and it can pay dividends to constantly revisit the question of whether the data contains everything sufficient to answer a particular question. Many problems in data science are much more easily solved by gathering the right data (or more of it) than by using fancier techniques.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;data-hygiene-and-data-splitting&quot;&gt;Data hygiene and data splitting&lt;&#x2F;h2&gt;
&lt;p&gt;If you frequently fit predictive models, you‚Äôll be aware of the risks of overfitting and the need to reserve partitions of the data to check that your findings truly generalise to unseen data. The same is true for the iterative types of EDA discussed in this article. The more detailed your analysis is, the higher the risk that insights gleaned in your EDA are false discoveries (aka statistical flukes). It is important that the confirmatory part of your analysis (prediction accuracy measurement or hypothesis testing) occurs on a different piece of data to your EDA.&lt;&#x2F;p&gt;
&lt;p&gt;A related problem that frequently arises in machine learning projects is where EDA is run as a preliminary step before creating training and test splits. If the result of EDA influences your model choices (it nearly always will if done properly), then you‚Äôve potentially reduced your test set‚Äôs ability to measure true out-of-sample error. So before you do anything, create a hygienic environment for your EDA by splitting your data, so that you don‚Äôt accidentally leak information from your test set into your model.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;creativity-and-the-pitfalls-of-the-data-frame-api&quot;&gt;Creativity and the pitfalls of the data frame API&lt;&#x2F;h2&gt;
&lt;blockquote&gt;
&lt;p&gt;This was the tendency of jobs to be adapted to tools, rather than adapting tools to jobs.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Silvan_Tomkins&quot;&gt;Silvan Tomkins&lt;&#x2F;a&gt;, &lt;em&gt;Computer Simulation of Personality: Frontier of Psychological Theory (1963)&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Almost all data analysis now begins with some form of data frame ‚Äî a tabular data format with columns of mixed types, where each row is a record. In Python and R, data analysis tooling has coalesced around the data frame object, which has been a huge convenience and productivity boost for the analyst. I wouldn‚Äôt for a second debate that this hasn‚Äôt been a positive development, but there is a risk here that EDA, because of the ease and uniformity of use of the tooling, becomes an exercise in applying boilerplate code. This creates a hidden creativity trap where the analysis can become narrowed by the range of uses supported by a particular set of tools. While such tools are extremely powerful when they are genuinely supporting you to develop your understanding of the data narrative, it‚Äôs important to avoid becoming too reliant on any single tool.&lt;&#x2F;p&gt;
&lt;p&gt;My experience is that it‚Äôs good to have familiarity with tooling at multiple levels of abstraction. Extremely high level interfaces to auto-generate certain types of exploratory analysis are very handy, and big time savers when they provide just what you need. However, the majority of EDA is more creative in nature and becoming expert with data manipulation tools like dplyr and pandas in combination with graphical tools like matplotlib and ggplot2 provides much finer control and fewer restrictions on your creativity.&lt;&#x2F;p&gt;
&lt;p&gt;The main point here is that exploratory data analysis can‚Äôt and shouldn‚Äôt be automated, because it is a process to support a human (you) to learn, and to do that well, there are few shortcuts.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;closing-thought-an-analogy-with-critical-reading-and-literary-analysis&quot;&gt;Closing thought: an analogy with critical reading and literary analysis&lt;&#x2F;h2&gt;
&lt;p&gt;Like all good blog posts, my thinking on EDA began on Twitter. In the process, &lt;a href=&quot;https:&#x2F;&#x2F;x.com&#x2F;kierisi&quot;&gt;Jesse Mostipak&lt;&#x2F;a&gt; &lt;a href=&quot;https:&#x2F;&#x2F;x.com&#x2F;kierisi&#x2F;status&#x2F;1350812374165565440?s=20&quot;&gt;made a great point&lt;&#x2F;a&gt; that teaching EDA effectively might share similarities &lt;a href=&quot;https:&#x2F;&#x2F;guides.library.harvard.edu&#x2F;sixreadinghabits&quot;&gt;to the way students are taught to interrogate literary texts&lt;&#x2F;a&gt;. I‚Äôd never considered EDA this way, but the analogy resonated strongly with me, and much of my thinking in this post owes a lot to being sent off in this direction, üôè thanks Jesse! There‚Äôs a lot to unpack in the analogy, and I have no training in critical reading so I can‚Äôt speak with any authority on that subject. Nevertheless, it seems that interrogating a text has broad similar to EDA in the sense of being driven by the goal of developing a deep understanding of a text.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;fs.blog&#x2F;how-to-read-a-book&#x2F;&quot;&gt;This article by the Farnham Street blog&lt;&#x2F;a&gt; summarises four levels of critical reading, originally proposed by Mortimer Adler. The final most analytical form of interrogation, called synotopical or comparative reading, hits on some of the themes I‚Äôve discussed already:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;This task is undertaken by identifying relevant passages, translating the terminology, framing and ordering the questions that need answering, defining the issues, and having a conversation with the responses.&lt;&#x2F;p&gt;
&lt;p&gt;The goal is not to achieve an overall understanding of any particular book, but rather to understand the subject and develop a deep fluency.&lt;&#x2F;p&gt;
&lt;p&gt;This is all about identifying and filling in your knowledge gaps.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;fs.blog&#x2F;how-to-read-a-book&#x2F;&quot;&gt;&lt;em&gt;Farnham Street blog&lt;&#x2F;em&gt;, How to Read a Book: The Ultimate Guide by Mortimer Adler&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Sounds familiar doesn‚Äôt it? Asking questions (of yourself), contextualising and framing, closing knowledge gaps and achieving fluency are all key parts of a successful EDA. What I‚Äôm most excited about here is that we can draw on the analytical framework of an existing and well-established discipline, as scaffolding to think about how we can make improvements to the way we teach and practice EDA. Again, full credit to Jesse for this idea.&lt;&#x2F;p&gt;
</content>
        
    </entry>
</feed>
